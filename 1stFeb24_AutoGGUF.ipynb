{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1P646NEg33BZy4BfLDNpTz0V0lwIU3CHu","timestamp":1706782820346},{"file_id":"1H61GtGmJyJSoR8JWndkb9cCCdTs4aUuA","timestamp":1704552150856},{"file_id":"1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD","timestamp":1704464730560}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["\n","# @markdown Quantization methods: `q2_k`, `q3_k_l`, `q3_k_m`, `q3_k_s`, `q4_0`, `q4_1`, `q4_k_m`, `q4_k_s`, `q5_0`, `q5_1`, `q5_k_m`, `q5_k_s`, `q6_k`, `q8_0`\n","\n","# @markdown ---\n","\n","# @markdown ### âš¡ Quantization parameters\n","MODEL_ID = \"mlabonne/NeuralBeagle14-7B\" # @param {type:\"string\"}\n","QUANTIZATION_METHODS = \"q4_k_m, q5_k_m\" # @param {type:\"string\"}\n","QUANTIZATION_METHODS = QUANTIZATION_METHODS.replace(\" \", \"\").split(\",\")\n","\n","# @markdown ---\n","\n","# @markdown ### ðŸ¤— Hugging Face Hub\n","username = \"sachintripathi\" # @param {type:\"string\"}\n","token = \"HF_TOKEN\" # @param {type:\"string\"}\n","\n","MODEL_NAME = MODEL_ID.split('/')[-1]\n","\n","# Install llama.cpp\n","!git clone https://github.com/ggerganov/llama.cpp\n","!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n","!pip install -r llama.cpp/requirements.txt\n","\n","# Download model\n","!git lfs install\n","!git clone https://huggingface.co/{MODEL_ID}\n","\n","# Convert to fp16\n","fp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\"\n","!python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile {fp16}\n","\n","# Quantize the model for each method in the QUANTIZATION_METHODS list\n","for method in QUANTIZATION_METHODS:\n","    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n","    !./llama.cpp/quantize {fp16} {qtype} {method}\n","\n","!pip install -q huggingface_hub\n","from huggingface_hub import create_repo, HfApi\n","from google.colab import userdata, runtime\n","\n","# Defined in the secrets tab in Google Colab\n","hf_token = userdata.get(token)\n","api = HfApi()\n","\n","# Create empty repo\n","create_repo(\n","    repo_id = f\"{username}/{MODEL_NAME}-GGUF\",\n","    repo_type=\"model\",\n","    exist_ok=True,\n","    token=hf_token\n",")\n","\n","# Upload gguf files\n","api.upload_folder(\n","    folder_path=MODEL_NAME,\n","    repo_id=f\"{username}/{MODEL_NAME}-GGUF\",\n","    allow_patterns=[\"*.gguf\",\"$.md\"],\n","    token=hf_token\n",")\n","\n","# Kill runtime\n","runtime.unassign()"],"metadata":{"id":"fD24jJxq7t3k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uNCfUJNGvpiR"},"execution_count":null,"outputs":[]}]}